
TRADES is a principled adversarial training framework that balances natural accuracy and robustness via a fixed trade-off parameter $\beta$, yet a single global value may be suboptimal across training stages and data samples. In this work, we propose \textbf{AdaTRADES}, an adaptive extension of TRADES that introduces dynamic $\beta$ scheduling based on training progress and model confidence, together with uncertainty-guided adversarial example selection. Our method further enables sample-wise $\beta$ adjustment, allowing the model to focus robustness regularization on harder and more uncertain examples while preserving clean accuracy on easier ones. From a theoretical perspective, AdaTRADES motivates the necessity of dynamic and sample-dependent trade-offs in adversarial training, extending the original TRADES formulation. Empirically, we expect AdaTRADES to achieve a better robustness--accuracy trade-off and faster convergence than vanilla TRADES by concentrating computational effort on informative adversarial perturbations.
